{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00bd193a-eadc-4a6e-b7e6-a999d70243d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DIFF] Current: 2025-09-17  | Previous: (none)\n",
      "\n",
      "[DIFF] Summary\n",
      "  Total current partitions: 1980\n",
      "  Total previous partitions: 0\n",
      "  New OK partitions: 244\n",
      "  Changed OK partitions: 0\n",
      "  Became EMPTY: 0\n",
      "  Became OK (from empty): 0\n",
      "  Disappeared (prev but not current): 0\n",
      "  Unchanged OK: 0\n",
      "\n",
      "[DIFF] Reports written to:\n",
      "  out\\bronze\\diffs\\ingest_date=2025-09-17\\new_ok.csv\n",
      "  out\\bronze\\diffs\\ingest_date=2025-09-17\\changed_ok.csv\n",
      "  out\\bronze\\diffs\\ingest_date=2025-09-17\\became_empty.csv\n",
      "  out\\bronze\\diffs\\ingest_date=2025-09-17\\became_ok.csv\n",
      "  out\\bronze\\diffs\\ingest_date=2025-09-17\\disappeared.csv\n",
      "  out\\bronze\\diffs\\ingest_date=2025-09-17\\unchanged_ok.csv\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import re, json, gzip, hashlib, time\n",
    "\n",
    "# --- config (override if your root isn't 'out') ---\n",
    "OUTPUT_DIR = Path(\"out\")  # <- change if needed\n",
    "\n",
    "# ========== helpers ==========\n",
    "def _token(parts, key):\n",
    "    for p in parts:\n",
    "        if isinstance(p, str) and p.startswith(f\"{key}=\"):\n",
    "            return p.split(\"=\", 1)[1]\n",
    "    return None\n",
    "\n",
    "def _sha256_of_gzip_uncompressed(p: Path, max_bytes=16*1024*1024):\n",
    "    \"\"\"Fast-ish hash for change detection: read up to 16MB uncompressed.\"\"\"\n",
    "    h = hashlib.sha256()\n",
    "    with gzip.open(p, \"rb\") as g:\n",
    "        while True:\n",
    "            chunk = g.read(1024*256)\n",
    "            if not chunk:\n",
    "                break\n",
    "            h.update(chunk)\n",
    "            max_bytes -= len(chunk)\n",
    "            if max_bytes <= 0:\n",
    "                break\n",
    "    return h.hexdigest()\n",
    "\n",
    "def _bronze_data_base(out_root: Path) -> Path:\n",
    "    return out_root / \"bronze\" / \"data\"\n",
    "\n",
    "def _list_ingest_dates(out_root: Path):\n",
    "    base = _bronze_data_base(out_root)\n",
    "    if not base.exists():\n",
    "        print(f\"[DIFF] Base not found: {base} — run your downloader first (or set OUTPUT_DIR).\")\n",
    "        return []\n",
    "    dates = set()\n",
    "    for p in base.rglob(\"ingest_date=*\"):\n",
    "        if p.is_dir():\n",
    "            d = p.name.split(\"=\",1)[1]\n",
    "            if re.fullmatch(r\"\\d{4}-\\d{2}-\\d{2}\", d):\n",
    "                dates.add(d)\n",
    "    return sorted(dates)\n",
    "\n",
    "def _load_partition_index(out_root: Path, ingest_date: str):\n",
    "    \"\"\"\n",
    "    Returns: {(profile_key, area_type_id): {\"status\": ok|empty|ok_no_meta|missing, \"sha\": str|None, \"rows\": int|None}}\n",
    "    \"\"\"\n",
    "    base = _bronze_data_base(out_root)\n",
    "    idx = {}\n",
    "    if not base.exists():\n",
    "        return idx\n",
    "\n",
    "    # authoritative: _meta.json\n",
    "    for meta_path in base.rglob(\"_meta.json\"):\n",
    "        parts = list(meta_path.parts)\n",
    "        pkey = _token(parts, \"profile_key\")\n",
    "        atid = _token(parts, \"area_type_id\")\n",
    "        idate = _token(parts, \"ingest_date\")\n",
    "        if not (pkey and atid and idate) or idate != ingest_date:\n",
    "            continue\n",
    "\n",
    "        csv_path = meta_path.parent / \"all_data_by_profile.csv.gz\"\n",
    "        try:\n",
    "            meta = json.loads(meta_path.read_text(encoding=\"utf-8\"))\n",
    "        except Exception:\n",
    "            meta = {}\n",
    "        is_empty = bool(meta.get(\"empty\"))\n",
    "        sha = meta.get(\"sha256_uncompressed\")\n",
    "        rows = meta.get(\"row_count_estimate\")\n",
    "        status = \"empty\" if is_empty else (\"ok\" if csv_path.exists() else \"missing\")\n",
    "        # if ok but sha missing, compute a quick one so we can diff\n",
    "        if status == \"ok\" and not sha and csv_path.exists():\n",
    "            sha = _sha256_of_gzip_uncompressed(csv_path)\n",
    "        idx[(pkey, int(atid))] = {\"status\": status, \"sha\": sha, \"rows\": rows}\n",
    "\n",
    "    # also consider csvs with no meta (older runs)\n",
    "    for csv_path in base.rglob(\"all_data_by_profile.csv.gz\"):\n",
    "        parts = list(csv_path.parts)\n",
    "        pkey = _token(parts, \"profile_key\")\n",
    "        atid = _token(parts, \"area_type_id\")\n",
    "        idate = _token(parts, \"ingest_date\")\n",
    "        if not (pkey and atid and idate) or idate != ingest_date:\n",
    "            continue\n",
    "        k = (pkey, int(atid))\n",
    "        if k not in idx:\n",
    "            idx[k] = {\n",
    "                \"status\": \"ok_no_meta\",\n",
    "                \"sha\": _sha256_of_gzip_uncompressed(csv_path),\n",
    "                \"rows\": None\n",
    "            }\n",
    "    return idx\n",
    "\n",
    "def diff_ingests(out_root: Path, cur_date: str = None, prev_date: str = None, write_reports=True):\n",
    "    dates = _list_ingest_dates(out_root)\n",
    "    if not dates:\n",
    "        return {\"error\": \"no_ingests_found\", \"message\": f\"No ingest_date folders under {_bronze_data_base(out_root)}\"}\n",
    "    if cur_date is None:\n",
    "        cur_date = dates[-1]\n",
    "    if prev_date is None:\n",
    "        prevs = [d for d in dates if d < cur_date]\n",
    "        prev_date = prevs[-1] if prevs else None\n",
    "\n",
    "    print(f\"[DIFF] Current: {cur_date}  | Previous: {prev_date or '(none)'}\")\n",
    "    cur_idx = _load_partition_index(out_root, cur_date)\n",
    "    prev_idx = _load_partition_index(out_root, prev_date) if prev_date else {}\n",
    "\n",
    "    cur_keys = set(cur_idx.keys())\n",
    "    prev_keys = set(prev_idx.keys())\n",
    "\n",
    "    new_ok = []\n",
    "    changed_ok = []\n",
    "    became_empty = []\n",
    "    became_ok = []\n",
    "    disappeared = []\n",
    "    unchanged_ok = []\n",
    "\n",
    "    # new or changed\n",
    "    for k in sorted(cur_keys):\n",
    "        cur = cur_idx[k]\n",
    "        if k not in prev_keys:\n",
    "            if cur[\"status\"].startswith(\"ok\"):\n",
    "                new_ok.append((k[0], k[1], cur[\"sha\"], cur[\"rows\"]))\n",
    "            # empty-but-new isn't actionable usually; skip\n",
    "        else:\n",
    "            prev = prev_idx[k]\n",
    "            if cur[\"status\"].startswith(\"ok\") and prev[\"status\"].startswith(\"ok\"):\n",
    "                if (cur[\"sha\"] and prev[\"sha\"] and cur[\"sha\"] != prev[\"sha\"]):\n",
    "                    changed_ok.append((k[0], k[1], prev[\"sha\"], cur[\"sha\"]))\n",
    "                else:\n",
    "                    unchanged_ok.append((k[0], k[1], cur[\"sha\"]))\n",
    "            elif prev[\"status\"].startswith(\"ok\") and cur[\"status\"] == \"empty\":\n",
    "                became_empty.append((k[0], k[1]))\n",
    "            elif prev[\"status\"] == \"empty\" and cur[\"status\"].startswith(\"ok\"):\n",
    "                became_ok.append((k[0], k[1], cur[\"sha\"], cur[\"rows\"]))\n",
    "\n",
    "    # disappeared (present before, missing now)\n",
    "    for k in sorted(prev_keys - cur_keys):\n",
    "        disappeared.append((k[0], k[1]))\n",
    "\n",
    "    # summary\n",
    "    print(\"\\n[DIFF] Summary\")\n",
    "    print(f\"  Total current partitions: {len(cur_keys)}\")\n",
    "    print(f\"  Total previous partitions: {len(prev_keys)}\")\n",
    "    print(f\"  New OK partitions: {len(new_ok)}\")\n",
    "    print(f\"  Changed OK partitions: {len(changed_ok)}\")\n",
    "    print(f\"  Became EMPTY: {len(became_empty)}\")\n",
    "    print(f\"  Became OK (from empty): {len(became_ok)}\")\n",
    "    print(f\"  Disappeared (prev but not current): {len(disappeared)}\")\n",
    "    print(f\"  Unchanged OK: {len(unchanged_ok)}\")\n",
    "\n",
    "    out = {\n",
    "        \"current_date\": cur_date,\n",
    "        \"previous_date\": prev_date,\n",
    "        \"counts\": {\n",
    "            \"new_ok\": len(new_ok),\n",
    "            \"changed_ok\": len(changed_ok),\n",
    "            \"became_empty\": len(became_empty),\n",
    "            \"became_ok\": len(became_ok),\n",
    "            \"disappeared\": len(disappeared),\n",
    "            \"unchanged_ok\": len(unchanged_ok),\n",
    "        },\n",
    "        \"new_ok\": new_ok,\n",
    "        \"changed_ok\": changed_ok,\n",
    "        \"became_empty\": became_empty,\n",
    "        \"became_ok\": became_ok,\n",
    "        \"disappeared\": disappeared,\n",
    "        \"unchanged_ok\": unchanged_ok,\n",
    "    }\n",
    "\n",
    "    if write_reports:\n",
    "        diffs_dir = OUTPUT_DIR / \"bronze\" / \"diffs\" / f\"ingest_date={cur_date}\"\n",
    "        diffs_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # write minimal CSVs without pandas\n",
    "        def _w(name, rows, hdr):\n",
    "            p = diffs_dir / name\n",
    "            with p.open(\"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(\",\".join(hdr) + \"\\n\")\n",
    "                for r in rows:\n",
    "                    f.write(\",\".join(\"\" if x is None else str(x) for x in r) + \"\\n\")\n",
    "            return p\n",
    "\n",
    "        p1 = _w(\"new_ok.csv\", new_ok, [\"profile_key\",\"area_type_id\",\"sha\",\"row_count_estimate\"])\n",
    "        p2 = _w(\"changed_ok.csv\", changed_ok, [\"profile_key\",\"area_type_id\",\"prev_sha\",\"cur_sha\"])\n",
    "        p3 = _w(\"became_empty.csv\", became_empty, [\"profile_key\",\"area_type_id\"])\n",
    "        p4 = _w(\"became_ok.csv\", became_ok, [\"profile_key\",\"area_type_id\",\"sha\",\"row_count_estimate\"])\n",
    "        p5 = _w(\"disappeared.csv\", disappeared, [\"profile_key\",\"area_type_id\"])\n",
    "        p6 = _w(\"unchanged_ok.csv\", unchanged_ok, [\"profile_key\",\"area_type_id\",\"sha\"])\n",
    "\n",
    "        print(\"\\n[DIFF] Reports written to:\")\n",
    "        for p in [p1,p2,p3,p4,p5,p6]:\n",
    "            print(\" \", p)\n",
    "\n",
    "    return out\n",
    "\n",
    "# ---- run it (auto-picks latest vs previous) ----\n",
    "diff_result = diff_ingests(OUTPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73b4ae4e-6b85-466f-9beb-7b59e948d224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_ok: 244\n",
      "  ('amr-local-indicators', 7, '4d18bf8800eb1a7cf71350056e679e5bcb58c1229eed86b243ef501f4bdc7b1f', 1088577)\n",
      "  ('amr-local-indicators', 15, '0e59a223bb3280bec772e2681f51272270941948a914675e5b265a7ae2a7cbe2', 26349)\n",
      "  ('amr-local-indicators', 66, 'ae17d3b426447a34c8c978d5b1c4bd6c1f7fe30ff790050df57cff72964bc06b', 421989)\n",
      "  ('amr-local-indicators', 118, '3df1da969ac00d708385d5b03930333d764e36ffcaaf00d7ff5546f0313a7099', 392732)\n",
      "  ('amr-local-indicators', 129, '9dd99077f488c48d7073af8550bb329a36646fd73e04ed9316f843ff52530d28', 2521)\n",
      "  ('amr-local-indicators', 221, '55e60bb9264ed63eeb443c284495108e8476b6aa3bbf54aa694bed6ba8324ec7', 1589)\n",
      "  ('cancerservices', 7, 'ed04397b3b5f59c238081d5f43decc60c95189785652922b0e0d32f41acc9275', 3276947)\n",
      "  ('cancerservices', 15, '2b0e168661d8cf9f89e10364341878129bbf103e72bdd06e6d867a955db2fa24', 2477)\n",
      "  ('cancerservices', 66, '26ec336f0f4adb86730c15d0b138ce0e7db5f933a02ec54af3ceccf7ccf8c343', 57472)\n",
      "  ('cancerservices', 204, 'c92931ee847cac78436f2899b3ae02fa96e673104d61e3c1a1c9a8246307fdd7', 661893)\n",
      "changed_ok: 0\n",
      "became_ok: 0\n",
      "became_empty: 0\n",
      "disappeared: 0\n"
     ]
    }
   ],
   "source": [
    "# show the exact partitions in each bucket\n",
    "for name in [\"new_ok\",\"changed_ok\",\"became_ok\",\"became_empty\",\"disappeared\"]:\n",
    "    rows = diff_result.get(name, [])\n",
    "    print(f\"{name}: {len(rows)}\")\n",
    "    for r in rows[:10]:\n",
    "        print(\" \", r)  # (profile_key, area_type_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "080692af-b0aa-41bd-8482-8ee01d4e26ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing local files: 0\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "def check_local_exists(items, today):\n",
    "    base = Path(\"out/bronze/data\")\n",
    "    missing = []\n",
    "    for item in items:\n",
    "        pkey, atid = item[0], int(item[1])\n",
    "        for name in [\"_meta.json\", \"all_data_by_profile.csv.gz\"]:\n",
    "            p = base / f\"profile_key={pkey}\" / f\"area_type_id={atid}\" / f\"ingest_date={today}\" / name\n",
    "            if not p.exists(): missing.append(str(p))\n",
    "    return missing\n",
    "\n",
    "to_upload = (diff_result.get(\"new_ok\", []) +\n",
    "             diff_result.get(\"changed_ok\", []) +\n",
    "             diff_result.get(\"became_ok\", []))\n",
    "missing_paths = check_local_exists(to_upload, diff_result[\"current_date\"])\n",
    "print(\"Missing local files:\", len(missing_paths))\n",
    "for m in missing_paths[:10]: print(\"  \", m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0f81510-0fb3-497b-9795-2e015908efa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "from boto3.s3.transfer import TransferConfig\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "\n",
    "def mime_for(path: Path):\n",
    "    name = path.name.lower()\n",
    "    if name.endswith(\".csv.gz\"):\n",
    "        return {\"ContentType\": \"text/csv\", \"ContentEncoding\": \"gzip\"}\n",
    "    if name.endswith(\".json.gz\"):\n",
    "        return {\"ContentType\": \"application/json\", \"ContentEncoding\": \"gzip\"}\n",
    "    if name.endswith(\".json\"):\n",
    "        return {\"ContentType\": \"application/json\"}\n",
    "    return {\"ContentType\": \"application/octet-stream\"}\n",
    "\n",
    "def iter_files_for_ingest(root: Path, ingest_date: str):\n",
    "    \"\"\"\n",
    "    Yield files under out/bronze/** that either:\n",
    "      - live in a folder with /ingest_date=<date>/ in its path; or\n",
    "      - are the manifest for that ingest date at out/bronze/manifest_ingest_date=<date>.json\n",
    "    \"\"\"\n",
    "    bronze_root = root / \"bronze\"\n",
    "    marker = f\"/ingest_date={ingest_date}/\"\n",
    "    for p in bronze_root.rglob(\"*\"):\n",
    "        if p.is_file():\n",
    "            # normalize to posix\n",
    "            posix = p.as_posix()\n",
    "            if marker in posix:\n",
    "                yield p\n",
    "    # include the manifest for that date if present\n",
    "    manifest = bronze_root / f\"manifest_ingest_date={ingest_date}.json\"\n",
    "    if manifest.exists():\n",
    "        yield manifest\n",
    "\n",
    "def build_session(profile: str | None, region: str | None):\n",
    "    if profile:\n",
    "        return boto3.Session(profile_name=profile, region_name=region)\n",
    "    return boto3.Session(region_name=region)\n",
    "\n",
    "def s3_key_for_local(prefix: str, out_root: Path, local_path: Path):\n",
    "    rel = local_path.relative_to(out_root).as_posix()  # \"bronze/...\"\n",
    "    prefix = prefix.strip(\"/\")\n",
    "    return f\"{prefix}/{rel}\" if prefix else rel\n",
    "\n",
    "def object_exists_same_size(s3_client, bucket: str, key: str, size: int) -> bool:\n",
    "    try:\n",
    "        resp = s3_client.head_object(Bucket=bucket, Key=key)\n",
    "        return resp.get(\"ContentLength\") == size\n",
    "    except ClientError as e:\n",
    "        if e.response.get(\"ResponseMetadata\", {}).get(\"HTTPStatusCode\") == 404:\n",
    "            return False\n",
    "        # if access denied or other, re-raise so user sees it\n",
    "        raise\n",
    "\n",
    "# ---------- Main upload ----------\n",
    "\n",
    "def upload_bronze_ingest(\n",
    "    bucket: str,\n",
    "    prefix: str,\n",
    "    ingest_date: str,\n",
    "    out_dir: str = \"out\",\n",
    "    aws_profile: str | None = None,\n",
    "    region: str | None = None,\n",
    "    workers: int = 8,\n",
    "    dry_run: bool = False,\n",
    "):\n",
    "    sess = build_session(aws_profile, region)\n",
    "    s3 = sess.client(\"s3\")\n",
    "    out_root = Path(out_dir).resolve()\n",
    "\n",
    "    if not (out_root / \"bronze\").exists():\n",
    "        raise SystemExit(f\"❌ Not found: {out_root/'bronze'}  (run your downloader first)\")\n",
    "\n",
    "    # Collect files\n",
    "    files = list(iter_files_for_ingest(out_root, ingest_date))\n",
    "    if not files:\n",
    "        raise SystemExit(f\"❌ No files found under {out_root}/bronze for ingest_date={ingest_date}\")\n",
    "\n",
    "    # Build upload plan (skip if already same size in S3)\n",
    "    plan = []\n",
    "    skipped_existing = 0\n",
    "    for p in files:\n",
    "        key = s3_key_for_local(prefix, out_root, p)\n",
    "        try:\n",
    "            if object_exists_same_size(s3, bucket, key, p.stat().st_size):\n",
    "                skipped_existing += 1\n",
    "                continue\n",
    "        except ClientError as e:\n",
    "            # surface credential/permission issues immediately\n",
    "            raise\n",
    "        plan.append((p, key))\n",
    "\n",
    "    print(f\"[PLAN] ingest_date={ingest_date}\")\n",
    "    print(f\"  Found local files: {len(files)}\")\n",
    "    print(f\"  Already present (same size) in S3: {skipped_existing}\")\n",
    "    print(f\"  To upload now: {len(plan)}\")\n",
    "    if dry_run:\n",
    "        for p, k in plan[:10]:\n",
    "            print(\"  ->\", p, \"→\", f\"s3://{bucket}/{k}\")\n",
    "        if len(plan) > 10:\n",
    "            print(f\"  ...and {len(plan)-10} more\")\n",
    "        print(\"Dry-run: exiting without changes.\")\n",
    "        return\n",
    "\n",
    "    # Transfer config\n",
    "    cfg = TransferConfig(\n",
    "        multipart_threshold=8*1024*1024,\n",
    "        multipart_chunksize=8*1024*1024,\n",
    "        max_concurrency=workers,\n",
    "        use_threads=True,\n",
    "    )\n",
    "\n",
    "    def _upload_one(local_path: Path, key: str):\n",
    "        extra = mime_for(local_path)\n",
    "        s3.upload_file(str(local_path), bucket, key, ExtraArgs=extra, Config=cfg)\n",
    "        return key\n",
    "\n",
    "    uploaded = 0\n",
    "    errors = 0\n",
    "    with ThreadPoolExecutor(max_workers=workers) as ex:\n",
    "        futs = [ex.submit(_upload_one, p, k) for (p, k) in plan]\n",
    "        for f in tqdm(as_completed(futs), total=len(futs), desc=\"S3 Upload\", leave=True):\n",
    "            try:\n",
    "                f.result()\n",
    "                uploaded += 1\n",
    "            except Exception as e:\n",
    "                errors += 1\n",
    "                print(\"[ERROR] upload failed:\", e)\n",
    "\n",
    "    print(f\"\\n✅ Upload complete: uploaded {uploaded}/{len(plan)}; skipped (already present) {skipped_existing}; errors {errors}\")\n",
    "    print(f\"   Destination prefix: s3://{bucket}/{prefix.strip('/')}/bronze/ ... (kept all historical ingest_date folders)\")\n",
    "\n",
    "# ---------- CLI ----------\n",
    "\n",
    "def parse_args():\n",
    "    ap = argparse.ArgumentParser(description=\"Upload a bronze ingest_date to S3 (no deletions).\")\n",
    "    ap.add_argument(\"--bucket\", required=True, help=\"S3 bucket name\")\n",
    "    ap.add_argument(\"--prefix\", default=\"\", help=\"S3 key prefix (e.g. 'my/project')\")\n",
    "    ap.add_argument(\"--ingest-date\", required=True, help=\"Ingest date to upload, e.g. 2025-09-15\")\n",
    "    ap.add_argument(\"--out-dir\", default=\"out\", help=\"Local output root (default: out)\")\n",
    "    ap.add_argument(\"--aws-profile\", default=os.environ.get(\"AWS_PROFILE\"), help=\"AWS CLI profile name\")\n",
    "    ap.add_argument(\"--region\", default=os.environ.get(\"AWS_REGION\") or os.environ.get(\"AWS_DEFAULT_REGION\"), help=\"AWS region\")\n",
    "    ap.add_argument(\"--workers\", type=int, default=8, help=\"Concurrent uploads (default: 8)\")\n",
    "    ap.add_argument(\"--dry-run\", action=\"store_true\", help=\"Show plan without uploading\")\n",
    "    return ap.parse_args()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2803e49b-14d5-4cb4-b8c8-b2967d3472c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auto-detected ingest_date: 2025-09-17\n",
      "[PLAN] ingest_date=2025-09-17\n",
      "  Found local files: 2329\n",
      "  Already present (same size) in S3: 0\n",
      "  To upload now: 2329\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b143b409ff364b469a84cb9ec8583c16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "S3 Upload:   0%|          | 0/2329 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Upload complete: uploaded 2329/2329; skipped (already present) 0; errors 0\n",
      "   Destination prefix: s3://test-nhs-fingertips//bronze/ ... (kept all historical ingest_date folders)\n"
     ]
    }
   ],
   "source": [
    "# no import needed if upload_bronze_ingest is already defined in this notebook\n",
    "from pathlib import Path\n",
    "\n",
    "def autodetect_ingest(out_dir=\"out\"):\n",
    "    root = Path(out_dir) / \"bronze\" / \"data\"\n",
    "    dates = sorted({p.name.split(\"=\",1)[1] for p in root.rglob(\"ingest_date=*\") if p.is_dir()})\n",
    "    if not dates:\n",
    "        raise SystemExit(f\"No ingest_date folders found under {root}\")\n",
    "    print(\"Auto-detected ingest_date:\", dates[-1])\n",
    "    return dates[-1]\n",
    "\n",
    "ingest_date = autodetect_ingest(\"out\")\n",
    "\n",
    "# Run the upload\n",
    "upload_bronze_ingest(\n",
    "    bucket=\"test-nhs-fingertips\",\n",
    "    prefix=\"\",                 # keep empty so keys start with 'bronze/...'\n",
    "    ingest_date=ingest_date,   # e.g. \"2025-09-15\"\n",
    "    out_dir=\"out\",\n",
    "    region=\"eu-west-2\",\n",
    "    aws_profile=\"mybronze\",    # or None if you rely on env/role\n",
    "    workers=8,\n",
    "    dry_run=False,             # try True first to preview\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18d41dc-7857-4656-a4d1-a9e6c148b865",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
