{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7750d3e1-1587-4ad3-9bd1-663996763775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dumping Bronze metadata…\n",
      "Planned tasks: 1980 from 45 profiles (AT cache misses: 0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1dfab8f9c144a7e9cc369afc4b5f1e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Bronze CSV:   0%|          | 0/1980 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main pass → OK:243  Empties:1736  Errors:1  (in 1:08:12)\n",
      "Error breakdown: http_5xx:1\n",
      "Failures to retry: 1\n",
      "Retrying pairs:\n",
      "  - profile_id=20 key=general-practice area_type_id=7\n",
      "[retry][general-practice/7] trying by_profile_id (no parent)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a217dc65c19344ddaa2432069d3b31ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Retry fallbacks:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[retry][general-practice/7] no-parent failed: http_5xx 500 500 Server Error: Internal Server Error for url: https://fingertips.phe.org.uk/api/all_data/csv/by_profile_id?profile_id=20&child_area_type_id=7\n",
      "[retry][general-practice/7] trying group concat (deadline 1800s)\n",
      "[retry][general-practice/7] groups 1/14 (used=1, rows~1309061)\n",
      "[retry][general-practice/7] groups 5/14 (used=5, rows~4459028)\n",
      "[retry][general-practice/7] groups 10/14 (used=10, rows~7961697)\n",
      "[retry][general-practice/7] timeout after 13/14 groups\n",
      "[retry][general-practice/7] success via groups used=13 rows~9647287\n",
      "Retry pass → OK:1  Empties:0  Errors:0  (in 44:00)\n",
      "\n",
      "Running Bronze completeness audit…\n",
      "Using INGEST_DATE: 2025-09-17\n",
      "Expected pairs (from cache): 1980\n",
      "Actual pairs with meta: 1980\n",
      "\n",
      "=== Completeness ===\n",
      "Expected pairs: 1980\n",
      "Covered OK (incl. ok_no_meta): 244\n",
      "Covered EMPTY: 1736\n",
      "Missing: 0\n",
      "Audit written to: out\\bronze\\audits\\ingest_date=2025-09-17\n",
      "\n",
      "Backfilling meta for CSV-only partitions (if any)…\n",
      "Backfilled meta files: 0\n",
      "Manifest written. Total elapsed: 1:56:16\n",
      "S3 upload disabled; set DO_UPLOAD_TO_S3=True and configure S3_BUCKET/S3_PREFIX to enable.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "fingertips_bronze_all_in_one.py\n",
    "\n",
    "Local downloader + S3 uploader for Fingertips \"bronze\" layer.\n",
    "\n",
    "Key upgrades:\n",
    "- Treat header-only at every stage as EMPTY (not ERROR) with clear reasons.\n",
    "- Fallback A: concat per-group CSVs (by_group_id) into one file.\n",
    "- Fallback B: concat per-indicator CSVs (by_indicator_id, batched) using Indicator Metadata snapshot.\n",
    "- Completed CSV-only meta backfill.\n",
    "- Robust metadata CSV parsing + mappings (profile→groups→indicators).\n",
    "\"\"\"\n",
    "\n",
    "import os, re, gzip, json, hashlib, datetime as dt, time, random, csv\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple, Iterable\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter, Retry\n",
    "from dateutil.tz import tzlocal\n",
    "from tqdm.auto import tqdm\n",
    "from urllib3.exceptions import ProtocolError as Urllib3ProtocolError\n",
    "from requests.exceptions import ConnectionError, ReadTimeout, ChunkedEncodingError, HTTPError\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "API_BASE = \"https://fingertips.phe.org.uk/api\"\n",
    "UA = \"fingertips-bronze-downloader/nb/4.0 (+dataops; mailto:you@example.com)\"\n",
    "\n",
    "OUTPUT_DIR = Path(\"out\")                       # local output root\n",
    "INGEST_DATE = dt.date.today().isoformat()      # partition date\n",
    "PARENT_AREA_TYPE_ID: Optional[int] = 15        # England as parent (safe default) — set None to omit\n",
    "\n",
    "# Concurrency & backoff\n",
    "MAX_WORKERS = 6                                # main pass concurrency; lower if server is flaky\n",
    "GENTLE_JITTER = (0.12, 0.28)                   # delay between task starts (seconds)\n",
    "\n",
    "# Retry controls\n",
    "RETRY_MAX_WORKERS = 4                          # retry concurrency\n",
    "RETRY_PER_TASK_DEADLINE_S = 1800               # 30 min per retry task hard cap\n",
    "RETRY_GROUP_LOG_EVERY = 5                      # log every N groups in group-concat fallback\n",
    "\n",
    "# Indicator fallback controls\n",
    "INDICATOR_FALLBACK_BATCH_SIZE = 25             # indicator_ids per request\n",
    "INDICATOR_FALLBACK_MAX_BATCHES = 300           # hard cap per task to avoid runaway (~7,500 indicators)\n",
    "INDICATOR_FALLBACK_LOG_EVERY = 5               # log cadence in batches\n",
    "\n",
    "# Scope\n",
    "ONLY_PROFILES: Optional[List[str]] = None      # e.g. [\"public-health-outcomes-framework\",\"local-health\"] or None for ALL\n",
    "\n",
    "# Metadata fetch\n",
    "DUMP_METADATA = True                           # dump lookups + indicator metadata CSV\n",
    "INCLUDE_AREAS_LISTS = True                     # dump areas_by_area_type lists (heavy but useful)\n",
    "\n",
    "# S3 upload controls\n",
    "DO_UPLOAD_TO_S3 = False                        # set True to upload\n",
    "S3_BUCKET = \"your-bucket-name\"\n",
    "S3_PREFIX = \"your/prefix\"                      # no leading/trailing slash handling below\n",
    "S3_CLEAN_INGEST_DATE_FIRST = False             # True = delete s3 keys for this ingest_date before uploading\n",
    "S3_MAX_UPLOAD_WORKERS = 8\n",
    "\n",
    "# Behavior toggles\n",
    "TREAT_ZERO_ROWS_IN_FALLBACK_AS_EMPTY = True    # mark as empty when groups/indicators have no rows\n",
    "OMIT_PARENT_ON_MAIN_IF_500 = False             # optional: try no-parent directly in main on server 5xx\n",
    "\n",
    "# Throttling (be nice to the API)\n",
    "THROTTLE_BEFORE_REQUEST = (0.15, 0.35)   # sleep before each HTTP call inside fallbacks\n",
    "THROTTLE_AFTER_SUCCESS  = (0.05, 0.15)   # short pause after a successful chunk\n",
    "THROTTLE_AFTER_FAILURE  = (0.8, 1.6)     # longer pause after a failed attempt\n",
    "\n",
    "# ---------- HTTP session ----------\n",
    "def make_session() -> requests.Session:\n",
    "    s = requests.Session()\n",
    "    s.headers.update({\"User-Agent\": UA, \"Connection\": \"close\"})\n",
    "    retries = Retry(\n",
    "        total=5,\n",
    "        backoff_factor=0.6,\n",
    "        status_forcelist=(429, 500, 502, 503, 504),\n",
    "        allowed_methods=frozenset([\"GET\"]),\n",
    "        raise_on_status=False,\n",
    "    )\n",
    "    s.mount(\"https://\", HTTPAdapter(max_retries=retries, pool_maxsize=50))\n",
    "    return s\n",
    "SESSION = make_session()\n",
    "\n",
    "# ---------- Utilities ----------\n",
    "def slugify(text: str) -> str:\n",
    "    text = (text or \"\").strip().lower()\n",
    "    text = re.sub(r\"[^a-z0-9]+\", \"-\", text)\n",
    "    text = re.sub(r\"-{2,}\", \"-\", text).strip(\"-\")\n",
    "    return text or \"unknown\"\n",
    "\n",
    "def now_iso():\n",
    "    return dt.datetime.now(tzlocal()).isoformat()\n",
    "\n",
    "def ensure_dir(p: Path):\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def write_json(path: Path, obj: dict):\n",
    "    ensure_dir(path.parent)\n",
    "    try:\n",
    "        import orjson as _oj\n",
    "        path.write_bytes(_oj.dumps(obj))\n",
    "    except Exception:\n",
    "        path.write_text(json.dumps(obj, ensure_ascii=False, separators=(\",\",\":\")))\n",
    "\n",
    "def fmt_secs(s: float) -> str:\n",
    "    if s < 60: return f\"{int(s)}s\"\n",
    "    m = int(s // 60); sec = int(s % 60)\n",
    "    if m < 60: return f\"{m:02d}:{sec:02d}\"\n",
    "    h = m // 60; m %= 60\n",
    "    return f\"{h:d}:{m:02d}:{sec:02d}\"\n",
    "\n",
    "# ---------- Error classification ----------\n",
    "def classify_exception(e: Exception) -> Tuple[str, Optional[int], str]:\n",
    "    \"\"\"\n",
    "    Returns (reason, status_code, detail)\n",
    "    reason ∈ {http_4xx,http_5xx,timeout,connection,protocol,other}\n",
    "    \"\"\"\n",
    "    if isinstance(e, HTTPError):\n",
    "        code = getattr(getattr(e, \"response\", None), \"status_code\", None)\n",
    "        if code and 400 <= code < 500:\n",
    "            return \"http_4xx\", code, str(e)\n",
    "        if code and code >= 500:\n",
    "            return \"http_5xx\", code, str(e)\n",
    "        return \"http_4xx\", code, str(e)\n",
    "    if isinstance(e, ReadTimeout):\n",
    "        return \"timeout\", None, str(e)\n",
    "    if isinstance(e, (ConnectionError, ChunkedEncodingError)):\n",
    "        return \"connection\", None, str(e)\n",
    "    if isinstance(e, Urllib3ProtocolError):\n",
    "        return \"protocol\", None, str(e)\n",
    "    return \"other\", None, f\"{type(e).__name__}: {e}\"\n",
    "\n",
    "# ---------- Robust HTTP helpers ----------\n",
    "def robust_get_json(url, params=None, max_tries=12, base_sleep=0.8):\n",
    "    \"\"\"\n",
    "    JSON GET with backoff + jitter. Final attempt uses a fresh one-off request.\n",
    "    \"\"\"\n",
    "    last_exc = None\n",
    "    for attempt in range(max_tries):\n",
    "        use_fresh = (attempt == max_tries - 1)\n",
    "        try:\n",
    "            if not use_fresh:\n",
    "                r = SESSION.get(url, params=params, headers={\"Connection\": \"close\"}, timeout=(10, 180))\n",
    "            else:\n",
    "                r = requests.get(url, params=params, headers={\"Connection\": \"close\", \"User-Agent\": UA}, timeout=(10, 180))\n",
    "            r.raise_for_status()\n",
    "            return r.json()\n",
    "        except (ConnectionError, ReadTimeout, ChunkedEncodingError, Urllib3ProtocolError, HTTPError) as e:\n",
    "            last_exc = e\n",
    "            if isinstance(e, HTTPError):\n",
    "                code = getattr(e.response, \"status_code\", None)\n",
    "                if code and code < 500:\n",
    "                    raise\n",
    "            if attempt < max_tries - 1:\n",
    "                time.sleep(base_sleep * (2 ** attempt) + random.random() * 0.6)\n",
    "            else:\n",
    "                raise last_exc\n",
    "\n",
    "def stream_csv_to_gzip(url: str, params: Dict[str, str], dest_path: Path) -> Tuple[int, str]:\n",
    "    \"\"\"Stream CSV to gzip; returns (row_count_estimate, sha256 of UNCOMPRESSED CSV).\"\"\"\n",
    "    ensure_dir(dest_path.parent)\n",
    "    sha = hashlib.sha256()\n",
    "    rows = 0\n",
    "    with SESSION.get(url, params=params, headers={\"Connection\": \"close\"}, stream=True, timeout=(10, 600)) as r:\n",
    "        r.raise_for_status()\n",
    "        with gzip.open(dest_path, \"wb\", compresslevel=6) as gz:\n",
    "            for chunk in r.iter_content(chunk_size=1024*256):\n",
    "                if not chunk:\n",
    "                    continue\n",
    "                gz.write(chunk)\n",
    "                sha.update(chunk)\n",
    "                rows += chunk.count(b\"\\n\")\n",
    "    return rows, sha.hexdigest()\n",
    "\n",
    "def robust_stream_csv_to_gzip(url, params, dest_path, max_tries=10, base_sleep=0.8):\n",
    "    \"\"\"\n",
    "    Wrapper that retries streaming; on final failure re-raises the *last* exception\n",
    "    and ensures partial file is removed.\n",
    "    \"\"\"\n",
    "    last_exc = None\n",
    "    for attempt in range(max_tries):\n",
    "        try:\n",
    "            return stream_csv_to_gzip(url, params, dest_path)\n",
    "        except (ConnectionError, ReadTimeout, ChunkedEncodingError, Urllib3ProtocolError, HTTPError) as e:\n",
    "            last_exc = e\n",
    "            try:\n",
    "                if dest_path.exists(): dest_path.unlink()\n",
    "            except Exception:\n",
    "                pass\n",
    "            if isinstance(e, HTTPError):\n",
    "                code = getattr(e.response, \"status_code\", None)\n",
    "                if code and code < 500:\n",
    "                    break\n",
    "            if attempt < max_tries - 1:\n",
    "                time.sleep(base_sleep * (2 ** attempt) + random.random() * 0.5)\n",
    "            else:\n",
    "                break\n",
    "    raise last_exc if last_exc else RuntimeError(\"unknown failure robust_stream_csv_to_gzip\")\n",
    "\n",
    "def _write_response_stream_to_gz(resp, gz, sha, skip_header=False):\n",
    "    \"\"\"\n",
    "    Stream an HTTP CSV response into open gzip. If skip_header=True, drop the first line.\n",
    "    Updates sha with UNCOMPRESSED bytes. Returns added row count.\n",
    "    \"\"\"\n",
    "    rows_added = 0\n",
    "    header_dropped = not skip_header\n",
    "    carry = b\"\"\n",
    "    for chunk in resp.iter_content(chunk_size=1024*256):\n",
    "        if not chunk:\n",
    "            continue\n",
    "        buf = carry + chunk\n",
    "        if not header_dropped and b\"\\n\" in buf:\n",
    "            i = buf.index(b\"\\n\") + 1\n",
    "            buf = buf[i:]\n",
    "            header_dropped = True\n",
    "        elif not header_dropped:\n",
    "            carry = buf\n",
    "            continue\n",
    "        else:\n",
    "            carry = b\"\"\n",
    "        if buf:\n",
    "            gz.write(buf)\n",
    "            sha.update(buf)\n",
    "            rows_added += buf.count(b\"\\n\")\n",
    "    return rows_added\n",
    "\n",
    "# ---------- Fingertips API wrappers ----------\n",
    "def fetch_profiles() -> List[dict]:\n",
    "    return robust_get_json(f\"{API_BASE}/profiles\")\n",
    "\n",
    "def fetch_all_area_types() -> List[dict]:\n",
    "    return robust_get_json(f\"{API_BASE}/area_types\")\n",
    "\n",
    "def fetch_area_types_for_profile(profile_id: int) -> List[dict]:\n",
    "    return robust_get_json(f\"{API_BASE}/area_types\", params={\"profile_id\": profile_id})\n",
    "\n",
    "def fetch_areas_by_type(area_type_id: int) -> List[dict]:\n",
    "    return robust_get_json(f\"{API_BASE}/areas/by_area_type\", params={\"area_type_id\": area_type_id})\n",
    "\n",
    "def fetch_ages() -> List[dict]:\n",
    "    return robust_get_json(f\"{API_BASE}/ages\")\n",
    "\n",
    "def fetch_sexes() -> List[dict]:\n",
    "    return robust_get_json(f\"{API_BASE}/sexes\")\n",
    "\n",
    "def fetch_value_notes() -> List[dict]:\n",
    "    return robust_get_json(f\"{API_BASE}/value_notes\")\n",
    "\n",
    "# ---------- Cached metadata helpers ----------\n",
    "def _latest_meta_dir(out_root: Path) -> Optional[Path]:\n",
    "    meta_root = out_root / \"bronze\" / \"metadata\"\n",
    "    if not meta_root.exists():\n",
    "        return None\n",
    "    candidates = sorted([p for p in meta_root.iterdir() if p.is_dir() and p.name.startswith(\"ingest_date=\")])\n",
    "    return candidates[-1] if candidates else None\n",
    "\n",
    "def load_cached_profiles(out_root: Path) -> list:\n",
    "    d = _latest_meta_dir(out_root)\n",
    "    if not d: return []\n",
    "    f = d / \"profiles.json\"\n",
    "    if not f.exists(): return []\n",
    "    try:\n",
    "        return json.loads(f.read_text(encoding=\"utf-8\"))\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "def load_cached_area_types_for_profile(out_root: Path, pid: int) -> list:\n",
    "    d = _latest_meta_dir(out_root)\n",
    "    if not d: return []\n",
    "    f = d / f\"area_types_profile={pid}.json\"\n",
    "    if not f.exists(): return []\n",
    "    try:\n",
    "        return json.loads(f.read_text(encoding=\"utf-8\"))\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "# ---------- Indicator metadata index (profile→groups→indicators) ----------\n",
    "_META_INDEX = None   # lazy-loaded\n",
    "\n",
    "def _norm(s: str) -> str:\n",
    "    return re.sub(r\"[^a-z0-9]+\", \"_\", (s or \"\").strip().lower()).strip(\"_\")\n",
    "\n",
    "def load_indicator_index(out_root: Path) -> dict:\n",
    "    \"\"\"\n",
    "    Returns dict:\n",
    "      {\n",
    "        \"by_profile\": { <profile_id>: set(indicator_id), ... },\n",
    "        \"by_profile_group\": { (<profile_id>, <group_id>): set(indicator_id), ... }\n",
    "      }\n",
    "    Uses latest bronze/metadata/*/indicator_metadata.csv.gz\n",
    "    \"\"\"\n",
    "    global _META_INDEX\n",
    "    if _META_INDEX is not None:\n",
    "        return _META_INDEX\n",
    "\n",
    "    mdir = _latest_meta_dir(out_root)\n",
    "    if not mdir:\n",
    "        _META_INDEX = {\"by_profile\": {}, \"by_profile_group\": {}}\n",
    "        return _META_INDEX\n",
    "    gz_path = mdir / \"indicator_metadata.csv.gz\"\n",
    "    if not gz_path.exists():\n",
    "        _META_INDEX = {\"by_profile\": {}, \"by_profile_group\": {}}\n",
    "        return _META_INDEX\n",
    "\n",
    "    by_profile: Dict[int, set] = {}\n",
    "    by_pg: Dict[Tuple[int, int], set] = {}\n",
    "\n",
    "    with gzip.open(gz_path, \"rt\", encoding=\"utf-8\", errors=\"ignore\", newline=\"\") as g:\n",
    "        rdr = csv.reader(g)\n",
    "        header = next(rdr, None)\n",
    "        if not header:\n",
    "            _META_INDEX = {\"by_profile\": {}, \"by_profile_group\": {}}\n",
    "            return _META_INDEX\n",
    "        cols = {_norm(c): i for i, c in enumerate(header)}\n",
    "        idx_indicator = cols.get(\"indicator_id\")\n",
    "        idx_profile = cols.get(\"profile_id\")\n",
    "        idx_group = cols.get(\"domain_id\", None)\n",
    "        if idx_group is None:\n",
    "            idx_group = cols.get(\"group_id\", None)\n",
    "\n",
    "        if idx_indicator is None or idx_profile is None:\n",
    "            _META_INDEX = {\"by_profile\": {}, \"by_profile_group\": {}}\n",
    "            return _META_INDEX\n",
    "\n",
    "        for row in rdr:\n",
    "            try:\n",
    "                ind = int(row[idx_indicator])\n",
    "                pid = int(row[idx_profile])\n",
    "                gid = int(row[idx_group]) if (idx_group is not None and row[idx_group]) else None\n",
    "            except Exception:\n",
    "                continue\n",
    "            by_profile.setdefault(pid, set()).add(ind)\n",
    "            if gid is not None:\n",
    "                by_pg.setdefault((pid, gid), set()).add(ind)\n",
    "\n",
    "    _META_INDEX = {\"by_profile\": by_profile, \"by_profile_group\": by_pg}\n",
    "    return _META_INDEX\n",
    "\n",
    "# ---------- Bronze metadata snapshot ----------\n",
    "def dump_bronze_metadata(out_root: Path, ingest_date: str, include_area_lists: bool=True):\n",
    "    meta_dir = out_root / \"bronze\" / \"metadata\" / f\"ingest_date={ingest_date}\"\n",
    "    ensure_dir(meta_dir)\n",
    "\n",
    "    # Profiles\n",
    "    profiles = []\n",
    "    try:\n",
    "        profiles = fetch_profiles()\n",
    "        write_json(meta_dir / \"profiles.json\", profiles)\n",
    "    except Exception as e:\n",
    "        print(\"[WARN] profiles fetch failed; continuing without profiles.json →\", e)\n",
    "\n",
    "    # Indicator metadata CSV\n",
    "    ind_csv = meta_dir / \"indicator_metadata.csv.gz\"\n",
    "    if not ind_csv.exists():\n",
    "        try:\n",
    "            url = f\"{API_BASE}/indicator_metadata/csv/all\"\n",
    "            robust_stream_csv_to_gzip(url, {}, ind_csv)\n",
    "        except Exception as e:\n",
    "            print(\"[WARN] indicator_metadata CSV fetch failed; skipping →\", e)\n",
    "\n",
    "    # Entities\n",
    "    for name, fn in [(\"ages.json\", fetch_ages), (\"sexes.json\", fetch_sexes), (\"value_notes.json\", fetch_value_notes)]:\n",
    "        try:\n",
    "            write_json(meta_dir / name, fn())\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] {name} fetch failed; skipping →\", e)\n",
    "\n",
    "    # Area types list\n",
    "    try:\n",
    "        at_all = fetch_all_area_types()\n",
    "        write_json(meta_dir / \"area_types.json\", at_all)\n",
    "    except Exception as e:\n",
    "        print(\"[WARN] area_types fetch failed; skipping →\", e)\n",
    "        at_all = []\n",
    "\n",
    "    # Areas by type (optional/heavy)\n",
    "    if include_area_lists and at_all:\n",
    "        ids = []\n",
    "        for a in at_all:\n",
    "            atid = a.get(\"Id\", a.get(\"AreaTypeId\"))\n",
    "            if isinstance(atid, int): ids.append(atid)\n",
    "        for atid in sorted(set(ids)):\n",
    "            try:\n",
    "                write_json(meta_dir / f\"areas_area_type_id={atid}.json\", fetch_areas_by_type(atid))\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "    # Per-profile area types (handy cache)\n",
    "    for p in profiles:\n",
    "        try:\n",
    "            ats = fetch_area_types_for_profile(p[\"Id\"])\n",
    "            write_json(meta_dir / f\"area_types_profile={p['Id']}.json\", ats)\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "# ---------- Download per (profile, area_type) ----------\n",
    "def _meta_base_paths(profile_key: str, area_type_id: int, ingest_date: str, out_root: Path):\n",
    "    base = (out_root / \"bronze\" / \"data\" /\n",
    "            f\"profile_key={profile_key}\" /\n",
    "            f\"area_type_id={area_type_id}\" /\n",
    "            f\"ingest_date={ingest_date}\")\n",
    "    return base, base / \"all_data_by_profile.csv.gz\", base / \"_meta.json\"\n",
    "\n",
    "def download_profile_area_csv(profile: dict, area_type_id: int, out_root: Path,\n",
    "                              ingest_date: str, parent_area_type_id: Optional[int]) -> dict:\n",
    "    \"\"\"\n",
    "    Returns dict with one of:\n",
    "      - {\"ok\": True, \"rows\": N, ...}\n",
    "      - {\"empty\": True, ...}\n",
    "      - {\"error\": \"...\", \"reason\": <category>, \"status\": <http code or None>, \"url\":..., \"params\":...}\n",
    "    \"\"\"\n",
    "    profile_id = profile[\"Id\"]\n",
    "    profile_key = profile.get(\"Key\") or slugify(profile[\"Name\"])\n",
    "    base, csv_path, meta_path = _meta_base_paths(profile_key, area_type_id, ingest_date, out_root)\n",
    "    ensure_dir(base)\n",
    "\n",
    "    # Idempotent skip (both OK and EMPTY)\n",
    "    if meta_path.exists():\n",
    "        try:\n",
    "            meta = json.loads(meta_path.read_text(encoding=\"utf-8\"))\n",
    "        except Exception:\n",
    "            meta = {}\n",
    "        if meta.get(\"empty\") is True:\n",
    "            return {\"skipped_empty\": True, \"profile_key\": profile_key, \"area_type_id\": area_type_id}\n",
    "        if csv_path.exists():\n",
    "            return {\"skipped\": True, \"profile_key\": profile_key, \"area_type_id\": area_type_id}\n",
    "\n",
    "    url = f\"{API_BASE}/all_data/csv/by_profile_id\"\n",
    "    params = {\n",
    "        \"profile_id\": str(profile_id),\n",
    "        \"child_area_type_id\": str(area_type_id),\n",
    "    }\n",
    "    if parent_area_type_id is not None:\n",
    "        params[\"parent_area_type_id\"] = str(parent_area_type_id)\n",
    "\n",
    "    try:\n",
    "        rows, sha = robust_stream_csv_to_gzip(url, params, csv_path)\n",
    "    except Exception as e:\n",
    "        # Optional immediate no-parent retry on 5xx (main pass)\n",
    "        if OMIT_PARENT_ON_MAIN_IF_500:\n",
    "            reason, code, detail = classify_exception(e)\n",
    "            if reason == \"http_5xx\":\n",
    "                try:\n",
    "                    params2 = {\"profile_id\": str(profile_id), \"child_area_type_id\": str(area_type_id)}\n",
    "                    rows, sha = robust_stream_csv_to_gzip(url, params2, csv_path)\n",
    "                    if rows <= 1:\n",
    "                        try: csv_path.unlink()\n",
    "                        except Exception: pass\n",
    "                        write_json(meta_path, {\n",
    "                            \"endpoint\": url, \"params\": params2,\n",
    "                            \"profile\": {\"Id\": profile_id, \"Name\": profile[\"Name\"], \"Key\": profile_key},\n",
    "                            \"area_type_id\": area_type_id, \"parent_area_type_id\": None,\n",
    "                            \"empty\": True, \"reason\": \"header-only (no data rows)\", \"fetched_at\": now_iso(),\n",
    "                            \"note\": \"main:used_no_parent\"\n",
    "                        })\n",
    "                        return {\"empty\": True, \"profile_key\": profile_key, \"area_type_id\": area_type_id}\n",
    "                    write_json(meta_path, {\n",
    "                        \"endpoint\": url, \"params\": params2,\n",
    "                        \"profile\": {\"Id\": profile_id, \"Name\": profile[\"Name\"], \"Key\": profile_key},\n",
    "                        \"area_type_id\": area_type_id, \"parent_area_type_id\": None,\n",
    "                        \"file\": csv_path.name, \"sha256_uncompressed\": sha,\n",
    "                        \"row_count_estimate\": rows, \"fetched_at\": now_iso(),\n",
    "                        \"note\": \"main:used_no_parent\"\n",
    "                    })\n",
    "                    return {\"ok\": True, \"profile_key\": profile_key, \"area_type_id\": area_type_id, \"rows\": rows}\n",
    "                except Exception:\n",
    "                    pass\n",
    "        # give error to retry logic\n",
    "        reason, code, detail = classify_exception(e)\n",
    "        return {\"error\": f\"download failed: {detail}\", \"reason\": reason, \"status\": code,\n",
    "                \"url\": url, \"params\": params,\n",
    "                \"profile_key\": profile_key, \"profile_id\": profile_id, \"area_type_id\": area_type_id}\n",
    "\n",
    "    if rows <= 1:\n",
    "        # header-only → record EMPTY and remove CSV\n",
    "        try: csv_path.unlink()\n",
    "        except Exception: pass\n",
    "        write_json(meta_path, {\n",
    "            \"endpoint\": url, \"params\": params,\n",
    "            \"profile\": {\"Id\": profile_id, \"Name\": profile[\"Name\"], \"Key\": profile_key},\n",
    "            \"area_type_id\": area_type_id, \"parent_area_type_id\": parent_area_type_id,\n",
    "            \"empty\": True, \"reason\": \"header-only (no data rows)\", \"fetched_at\": now_iso()\n",
    "        })\n",
    "        return {\"empty\": True, \"profile_key\": profile_key, \"area_type_id\": area_type_id}\n",
    "\n",
    "    write_json(meta_path, {\n",
    "        \"endpoint\": url, \"params\": params,\n",
    "        \"profile\": {\"Id\": profile_id, \"Name\": profile[\"Name\"], \"Key\": profile_key},\n",
    "        \"area_type_id\": area_type_id, \"parent_area_type_id\": parent_area_type_id,\n",
    "        \"file\": csv_path.name, \"sha256_uncompressed\": sha,\n",
    "        \"row_count_estimate\": rows, \"fetched_at\": now_iso()\n",
    "    })\n",
    "    return {\"ok\": True, \"profile_key\": profile_key, \"area_type_id\": area_type_id, \"rows\": rows}\n",
    "\n",
    "# ---------- Fallback A: concat groups ----------\n",
    "def robust_concat_groups_to_single_gzip(profile: dict,\n",
    "                                        area_type_id: int,\n",
    "                                        parent_area_type_id: Optional[int],\n",
    "                                        dest_path: Path,\n",
    "                                        deadline_ts: Optional[float] = None,\n",
    "                                        log_every: int = 5) -> Tuple[int, str, List[int]]:\n",
    "    ensure_dir(dest_path.parent)\n",
    "    groups = profile.get(\"GroupIds\") or []\n",
    "    if not groups:\n",
    "        return 0, \"\", []\n",
    "\n",
    "    sha = hashlib.sha256()\n",
    "    total_rows = 0\n",
    "    used: List[int] = []\n",
    "    key = (profile.get(\"Key\") or slugify(profile[\"Name\"]))\n",
    "    n = len(groups)\n",
    "\n",
    "    with gzip.open(dest_path, \"wb\", compresslevel=6) as gz:\n",
    "        first = True\n",
    "        for i, gid in enumerate(groups, start=1):\n",
    "            if deadline_ts and time.time() >= deadline_ts:\n",
    "                print(f\"[retry][{key}/{area_type_id}] timeout after {i-1}/{n} groups\")\n",
    "                break\n",
    "\n",
    "            url = f\"{API_BASE}/all_data/csv/by_group_id\"\n",
    "            params = {\"group_id\": str(gid), \"child_area_type_id\": str(area_type_id)}\n",
    "            if parent_area_type_id is not None:\n",
    "                params[\"parent_area_type_id\"] = str(parent_area_type_id)\n",
    "\n",
    "            # gentle throttle before each group request\n",
    "            time.sleep(random.uniform(*THROTTLE_BEFORE_REQUEST))\n",
    "\n",
    "            for attempt in range(6):\n",
    "                try:\n",
    "                    with SESSION.get(url, params=params, headers={\"Connection\": \"close\"},\n",
    "                                     stream=True, timeout=(10, 600)) as r:\n",
    "                        r.raise_for_status()\n",
    "                        added = _write_response_stream_to_gz(r, gz, sha, skip_header=not first)\n",
    "                        if added > 1:\n",
    "                            total_rows += added\n",
    "                            used.append(gid)\n",
    "                    # tiny breather on success\n",
    "                    time.sleep(random.uniform(*THROTTLE_AFTER_SUCCESS))\n",
    "                    break\n",
    "                except (ConnectionError, ReadTimeout, ChunkedEncodingError, Urllib3ProtocolError, HTTPError) as e:\n",
    "                    # exponential backoff + extra pause to be nice after failures\n",
    "                    if attempt < 5:\n",
    "                        time.sleep(0.6 * (2 ** attempt) + random.random() * 0.5)\n",
    "                        time.sleep(random.uniform(*THROTTLE_AFTER_FAILURE))\n",
    "                    else:\n",
    "                        print(f\"[retry][{key}/{area_type_id}] group {gid} failed permanently: {e}\")\n",
    "\n",
    "            if (i == 1) or (i % log_every == 0):\n",
    "                print(f\"[retry][{key}/{area_type_id}] groups {i}/{n} (used={len(used)}, rows~{total_rows})\")\n",
    "\n",
    "            first = False\n",
    "\n",
    "    return total_rows, sha.hexdigest(), used\n",
    "\n",
    "\n",
    "\n",
    "# ---------- Fallback B: concat indicators (from metadata index) ----------\n",
    "def iter_indicator_batches(indicator_ids: Iterable[int], batch_size: int) -> Iterable[List[int]]:\n",
    "    batch: List[int] = []\n",
    "    for i in indicator_ids:\n",
    "        batch.append(int(i))\n",
    "        if len(batch) >= batch_size:\n",
    "            yield batch\n",
    "            batch = []\n",
    "    if batch:\n",
    "        yield batch\n",
    "\n",
    "def robust_concat_indicators_to_single_gzip(profile: dict,\n",
    "                                            area_type_id: int,\n",
    "                                            parent_area_type_id: Optional[int],\n",
    "                                            dest_path: Path,\n",
    "                                            indicator_ids: List[int],\n",
    "                                            deadline_ts: Optional[float] = None,\n",
    "                                            log_every_batches: int = 5,\n",
    "                                            batch_size: int = 25,\n",
    "                                            max_batches: int = 300) -> Tuple[int, str, List[int]]:\n",
    "    ensure_dir(dest_path.parent)\n",
    "    if not indicator_ids:\n",
    "        return 0, \"\", []\n",
    "\n",
    "    sha = hashlib.sha256()\n",
    "    total_rows = 0\n",
    "    used: List[int] = []\n",
    "    key = (profile.get(\"Key\") or slugify(profile[\"Name\"]))\n",
    "\n",
    "    with gzip.open(dest_path, \"wb\", compresslevel=6) as gz:\n",
    "        first = True\n",
    "        for b_idx, batch in enumerate(iter_indicator_batches(indicator_ids, batch_size), start=1):\n",
    "            if b_idx > max_batches:\n",
    "                print(f\"[retry][{key}/{area_type_id}] indicator concat hit max_batches={max_batches}\")\n",
    "                break\n",
    "            if deadline_ts and time.time() >= deadline_ts:\n",
    "                print(f\"[retry][{key}/{area_type_id}] indicator concat deadline reached after {b_idx-1} batches\")\n",
    "                break\n",
    "\n",
    "            url = f\"{API_BASE}/all_data/csv/by_indicator_id\"\n",
    "            params = {\"indicator_ids\": \",\".join(str(x) for x in batch),\n",
    "                      \"child_area_type_id\": str(area_type_id)}\n",
    "            if parent_area_type_id is not None:\n",
    "                params[\"parent_area_type_id\"] = str(parent_area_type_id)\n",
    "\n",
    "            # gentle throttle before each indicator batch\n",
    "            time.sleep(random.uniform(*THROTTLE_BEFORE_REQUEST))\n",
    "\n",
    "            for attempt in range(6):\n",
    "                try:\n",
    "                    with SESSION.get(url, params=params, headers={\"Connection\": \"close\"},\n",
    "                                     stream=True, timeout=(10, 600)) as r:\n",
    "                        r.raise_for_status()\n",
    "                        added = _write_response_stream_to_gz(r, gz, sha, skip_header=not first)\n",
    "                        if added > 1:\n",
    "                            total_rows += added\n",
    "                            used.extend(batch)\n",
    "                    # tiny breather on success\n",
    "                    time.sleep(random.uniform(*THROTTLE_AFTER_SUCCESS))\n",
    "                    break\n",
    "                except (ConnectionError, ReadTimeout, ChunkedEncodingError, Urllib3ProtocolError, HTTPError) as e:\n",
    "                    if attempt < 5:\n",
    "                        time.sleep(0.6 * (2 ** attempt) + random.random() * 0.5)\n",
    "                        time.sleep(random.uniform(*THROTTLE_AFTER_FAILURE))\n",
    "                    else:\n",
    "                        print(f\"[retry][{key}/{area_type_id}] indicator batch failed permanently: {e}\")\n",
    "\n",
    "            if (b_idx == 1) or (b_idx % log_every_batches == 0):\n",
    "                print(f\"[retry][{key}/{area_type_id}] indicator batches {b_idx} (used≈{len(used)}, rows~{total_rows})\")\n",
    "\n",
    "            first = False\n",
    "\n",
    "    return total_rows, sha.hexdigest(), used\n",
    "\n",
    "\n",
    "def recover_profile_area_into_same_file(profile: dict, area_type_id: int, out_root: Path,\n",
    "                                        ingest_date: str, parent_area_type_id: Optional[int]) -> dict:\n",
    "    \"\"\"\n",
    "    Fallbacks for stubborn pairs:\n",
    "      1) by_profile_id WITHOUT parent_area_type_id\n",
    "      2) concatenate all by_group_id CSVs into ONE (header deduped) with per-task deadline\n",
    "      3) concatenate by_indicator_id (batched) using Indicator Metadata index\n",
    "    Returns one of ok/empty/error dicts (like main).\n",
    "    \"\"\"\n",
    "    key = (profile.get(\"Key\") or slugify(profile[\"Name\"]))\n",
    "    base, csv_path, meta_path = _meta_base_paths(key, area_type_id, ingest_date, out_root)\n",
    "    ensure_dir(base)\n",
    "\n",
    "    # honor prior empties/ok\n",
    "    if meta_path.exists():\n",
    "        try:\n",
    "            meta = json.loads(meta_path.read_text(encoding=\"utf-8\"))\n",
    "        except Exception:\n",
    "            meta = {}\n",
    "        if meta.get(\"empty\") is True:\n",
    "            print(f\"[retry][{key}/{area_type_id}] skipped (already empty in meta)\")\n",
    "            return {\"skipped_empty\": True, \"profile_key\": key, \"area_type_id\": area_type_id}\n",
    "        if csv_path.exists():\n",
    "            print(f\"[retry][{key}/{area_type_id}] skipped (csv already exists)\")\n",
    "            return {\"skipped\": True, \"profile_key\": key, \"area_type_id\": area_type_id}\n",
    "\n",
    "    # A) by_profile_id WITHOUT parent\n",
    "    url = f\"{API_BASE}/all_data/csv/by_profile_id\"\n",
    "    params_no_parent = {\"profile_id\": str(profile[\"Id\"]), \"child_area_type_id\": str(area_type_id)}\n",
    "    print(f\"[retry][{key}/{area_type_id}] trying by_profile_id (no parent)\")\n",
    "    try:\n",
    "        rows, sha = robust_stream_csv_to_gzip(url, params_no_parent, csv_path)\n",
    "        if rows > 1:\n",
    "            write_json(meta_path, {\n",
    "                \"endpoint\": url, \"params\": params_no_parent,\n",
    "                \"profile\": {\"Id\": profile[\"Id\"], \"Name\": profile[\"Name\"], \"Key\": key},\n",
    "                \"area_type_id\": area_type_id, \"parent_area_type_id\": None,\n",
    "                \"file\": csv_path.name, \"sha256_uncompressed\": sha,\n",
    "                \"row_count_estimate\": rows, \"fetched_at\": now_iso(), \"note\": \"fallback:no-parent\"\n",
    "            })\n",
    "            print(f\"[retry][{key}/{area_type_id}] success via no-parent rows~{rows}\")\n",
    "            return {\"ok\": True, \"profile_key\": key, \"area_type_id\": area_type_id, \"rows\": rows}\n",
    "        else:\n",
    "            try: csv_path.unlink()\n",
    "            except Exception: pass\n",
    "            write_json(meta_path, {\n",
    "                \"endpoint\": url, \"params\": params_no_parent,\n",
    "                \"profile\": {\"Id\": profile[\"Id\"], \"Name\": profile[\"Name\"], \"Key\": key},\n",
    "                \"area_type_id\": area_type_id, \"parent_area_type_id\": None,\n",
    "                \"empty\": True, \"reason\": \"header-only after no-parent\", \"fetched_at\": now_iso()\n",
    "            })\n",
    "            print(f\"[retry][{key}/{area_type_id}] empty via no-parent\")\n",
    "            return {\"empty\": True, \"profile_key\": key, \"area_type_id\": area_type_id}\n",
    "    except Exception as e:\n",
    "        reason, code, detail = classify_exception(e)\n",
    "        print(f\"[retry][{key}/{area_type_id}] no-parent failed: {reason} {code or ''} {detail}\")\n",
    "\n",
    "    # B) group concat with deadline\n",
    "    try:\n",
    "        if csv_path.exists():\n",
    "            csv_path.unlink()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    deadline_ts = time.time() + RETRY_PER_TASK_DEADLINE_S\n",
    "    print(f\"[retry][{key}/{area_type_id}] trying group concat (deadline {RETRY_PER_TASK_DEADLINE_S}s)\")\n",
    "    rows2, sha2, groups_used = robust_concat_groups_to_single_gzip(\n",
    "        profile, area_type_id, parent_area_type_id, csv_path,\n",
    "        deadline_ts=deadline_ts, log_every=RETRY_GROUP_LOG_EVERY\n",
    "    )\n",
    "    if rows2 > 1 and groups_used:\n",
    "        write_json(meta_path, {\n",
    "            \"endpoint\": f\"{API_BASE}/all_data/csv/by_group_id\",\n",
    "            \"params\": {\"group_ids\": groups_used, \"child_area_type_id\": area_type_id,\n",
    "                       \"parent_area_type_id\": parent_area_type_id},\n",
    "            \"profile\": {\"Id\": profile[\"Id\"], \"Name\": profile[\"Name\"], \"Key\": key},\n",
    "            \"area_type_id\": area_type_id,\n",
    "            \"file\": csv_path.name, \"sha256_uncompressed\": sha2,\n",
    "            \"row_count_estimate\": rows2, \"fetched_at\": now_iso(),\n",
    "            \"note\": \"fallback:combined_from_groups\"\n",
    "        })\n",
    "        print(f\"[retry][{key}/{area_type_id}] success via groups used={len(groups_used)} rows~{rows2}\")\n",
    "        return {\"ok\": True, \"profile_key\": key, \"area_type_id\": area_type_id,\n",
    "                \"rows\": rows2, \"note\": \"combined_from_groups\"}\n",
    "\n",
    "    # C) indicator concat fallback using metadata index\n",
    "    try:\n",
    "        if csv_path.exists():\n",
    "            csv_path.unlink()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    idx = load_indicator_index(out_root)\n",
    "    pid = profile[\"Id\"]\n",
    "    indicator_ids = sorted(idx.get(\"by_profile\", {}).get(pid, []))\n",
    "    if indicator_ids:\n",
    "        print(f\"[retry][{key}/{area_type_id}] trying indicator concat (batches of {INDICATOR_FALLBACK_BATCH_SIZE})\")\n",
    "        rows3, sha3, inds_used = robust_concat_indicators_to_single_gzip(\n",
    "            profile, area_type_id, parent_area_type_id, csv_path,\n",
    "            indicator_ids=indicator_ids,\n",
    "            deadline_ts=deadline_ts,\n",
    "            log_every_batches=INDICATOR_FALLBACK_LOG_EVERY,\n",
    "            batch_size=INDICATOR_FALLBACK_BATCH_SIZE,\n",
    "            max_batches=INDICATOR_FALLBACK_MAX_BATCHES\n",
    "        )\n",
    "        if rows3 > 1 and inds_used:\n",
    "            write_json(meta_path, {\n",
    "                \"endpoint\": f\"{API_BASE}/all_data/csv/by_indicator_id\",\n",
    "                \"params\": {\"indicator_ids_used_count\": len(inds_used), \"child_area_type_id\": area_type_id,\n",
    "                           \"parent_area_type_id\": parent_area_type_id},\n",
    "                \"profile\": {\"Id\": profile[\"Id\"], \"Name\": profile[\"Name\"], \"Key\": key},\n",
    "                \"area_type_id\": area_type_id,\n",
    "                \"file\": csv_path.name, \"sha256_uncompressed\": sha3,\n",
    "                \"row_count_estimate\": rows3, \"fetched_at\": now_iso(),\n",
    "                \"note\": \"fallback:combined_from_indicators\"\n",
    "            })\n",
    "            print(f\"[retry][{key}/{area_type_id}] success via indicators used≈{len(inds_used)} rows~{rows3}\")\n",
    "            return {\"ok\": True, \"profile_key\": key, \"area_type_id\": area_type_id,\n",
    "                    \"rows\": rows3, \"note\": \"combined_from_indicators\"}\n",
    "\n",
    "    # No rows from groups or indicators → treat as EMPTY if configured\n",
    "    if TREAT_ZERO_ROWS_IN_FALLBACK_AS_EMPTY:\n",
    "        try:\n",
    "            if csv_path.exists(): csv_path.unlink()\n",
    "        except Exception:\n",
    "            pass\n",
    "        write_json(meta_path, {\n",
    "            \"endpoint\": \"multiple\",\n",
    "            \"params\": {\"attempts\": [\"by_profile_id(no_parent)\", \"by_group_id(*groups)\", \"by_indicator_id(*batched)\"]},\n",
    "            \"profile\": {\"Id\": profile[\"Id\"], \"Name\": profile[\"Name\"], \"Key\": key},\n",
    "            \"area_type_id\": area_type_id, \"parent_area_type_id\": parent_area_type_id,\n",
    "            \"empty\": True, \"reason\": \"no rows from groups/indicators\", \"fetched_at\": now_iso()\n",
    "        })\n",
    "        print(f\"[retry][{key}/{area_type_id}] marked EMPTY after all fallbacks (no rows)\")\n",
    "        return {\"empty\": True, \"profile_key\": key, \"area_type_id\": area_type_id}\n",
    "\n",
    "    print(f\"[retry][{key}/{area_type_id}] fallbacks failed\")\n",
    "    return {\"error\": f\"fallbacks_failed for profile_id={profile['Id']}, area_type_id={area_type_id}\",\n",
    "            \"reason\": \"fallbacks_failed\", \"status\": None,\n",
    "            \"profile_key\": key, \"profile_id\": profile[\"Id\"], \"area_type_id\": area_type_id}\n",
    "\n",
    "# ---------- Bronze completeness audit ----------\n",
    "def bronze_completeness_audit(out_root: Path, ingest_date: Optional[str]) -> dict:\n",
    "    print(\"\\nRunning Bronze completeness audit…\")\n",
    "    data_root = out_root / \"bronze\" / \"data\"\n",
    "    ingest_dirs = sorted({p for p in data_root.rglob(\"ingest_date=*\") if p.is_dir()})\n",
    "    if ingest_date is None:\n",
    "        ingest_date = ingest_dirs[-1].name.split(\"=\",1)[1] if ingest_dirs else None\n",
    "    print(\"Using INGEST_DATE:\", ingest_date or \"(none found)\")\n",
    "\n",
    "    # load cached meta\n",
    "    meta_root = out_root / \"bronze\" / \"metadata\"\n",
    "    def latest_meta_dir():\n",
    "        ds = sorted([p for p in meta_root.glob(\"ingest_date=*\") if p.is_dir()])\n",
    "        return ds[-1] if ds else None\n",
    "\n",
    "    profiles, at_by_profile = [], {}\n",
    "    m_dir = latest_meta_dir()\n",
    "    if m_dir and (m_dir/\"profiles.json\").exists():\n",
    "        profiles = json.loads((m_dir/\"profiles.json\").read_text(encoding=\"utf-8\"))\n",
    "    if m_dir:\n",
    "        for f in m_dir.glob(\"area_types_profile=*.json\"):\n",
    "            try:\n",
    "                pid = int(f.stem.split(\"=\",1)[1])\n",
    "                at_by_profile[pid] = json.loads(f.read_text(encoding=\"utf-8\"))\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    def token(parts, key):\n",
    "        for p in parts:\n",
    "            if isinstance(p, str) and p.startswith(f\"{key}=\"):\n",
    "                return p.split(\"=\",1)[1]\n",
    "        return None\n",
    "\n",
    "    def scan_bronze_pairs(out_root: Path, ingest_date: str = None):\n",
    "        base = out_root / \"bronze\" / \"data\"\n",
    "        rec = []\n",
    "        for meta_path in base.rglob(\"_meta.json\"):\n",
    "            parts = list(meta_path.parts)\n",
    "            pkey = token(parts, \"profile_key\")\n",
    "            atid = token(parts, \"area_type_id\")\n",
    "            idate = token(parts, \"ingest_date\")\n",
    "            if not (pkey and atid and idate):\n",
    "                continue\n",
    "            if ingest_date and idate != ingest_date:\n",
    "                continue\n",
    "            try:\n",
    "                meta = json.loads(meta_path.read_text(encoding=\"utf-8\"))\n",
    "            except Exception:\n",
    "                meta = {}\n",
    "            status = \"empty\" if meta.get(\"empty\") else \"ok\"\n",
    "            rec.append((pkey, int(atid), idate, status, meta_path))\n",
    "        return rec\n",
    "\n",
    "    # Also treat CSV-only (no meta) as ok_no_meta\n",
    "    def scan_csv_without_meta(out_root: Path, ingest_date: str = None):\n",
    "        base = out_root / \"bronze\" / \"data\"\n",
    "        rec = []\n",
    "        for csv_path in base.rglob(\"all_data_by_profile.csv.gz\"):\n",
    "            parts = list(csv_path.parts)\n",
    "            pkey = token(parts, \"profile_key\")\n",
    "            atid = token(parts, \"area_type_id\")\n",
    "            idate = token(parts, \"ingest_date\")\n",
    "            if not (pkey and atid and idate):\n",
    "                continue\n",
    "            if ingest_date and idate != ingest_date:\n",
    "                continue\n",
    "            meta_path = csv_path.parent / \"_meta.json\"\n",
    "            if not meta_path.exists():\n",
    "                rec.append((pkey, int(atid), idate, \"ok_no_meta\", csv_path))\n",
    "        return rec\n",
    "\n",
    "    # Build expected (profile_key, area_type_id) pairs from cached metadata\n",
    "    def _slug(x: str) -> str:\n",
    "        return slugify(x)\n",
    "\n",
    "    expected_pairs = []\n",
    "    for p in profiles:\n",
    "        pkey = (p.get(\"Key\") or _slug(p[\"Name\"])).lower()\n",
    "        ats = at_by_profile.get(p[\"Id\"], [])\n",
    "        ids = []\n",
    "        for a in ats:\n",
    "            atid = a.get(\"Id\", a.get(\"AreaTypeId\"))\n",
    "            if isinstance(atid, int):\n",
    "                ids.append(atid)\n",
    "        for atid in sorted(set(ids)):\n",
    "            expected_pairs.append((pkey, p[\"Id\"], atid))\n",
    "    print(\"Expected pairs (from cache):\", len(expected_pairs))\n",
    "\n",
    "    actual = scan_bronze_pairs(out_root, ingest_date)\n",
    "    print(\"Actual pairs with meta:\", len(actual))\n",
    "\n",
    "    csv_only = scan_csv_without_meta(out_root, ingest_date)\n",
    "    if csv_only:\n",
    "        print(\"CSV-only pairs (no meta):\", len(csv_only))\n",
    "\n",
    "    actual_idx = {(pkey, atid): status for (pkey, atid, _, status, _) in actual}\n",
    "    for (pkey, atid, _, status, _) in csv_only:\n",
    "        actual_idx.setdefault((pkey, atid), status)\n",
    "\n",
    "    if expected_pairs:\n",
    "        expected_keys = {(pkey, atid) for (pkey, _, atid) in expected_pairs}\n",
    "    else:\n",
    "        expected_keys = set(actual_idx.keys())\n",
    "\n",
    "    covered_ok, covered_empty, missing = [], [], []\n",
    "    for (pkey, pid, atid) in (expected_pairs or [(p, None, a) for (p, a) in expected_keys]):\n",
    "        st = actual_idx.get((pkey, atid))\n",
    "        if st is None:\n",
    "            missing.append((pkey, pid, atid))\n",
    "        elif st == \"empty\":\n",
    "            covered_empty.append((pkey, pid, atid))\n",
    "        else:\n",
    "            covered_ok.append((pkey, pid, atid))\n",
    "\n",
    "    print(\"\\n=== Completeness ===\")\n",
    "    print(\"Expected pairs:\", len(expected_keys))\n",
    "    print(\"Covered OK (incl. ok_no_meta):\", len(covered_ok))\n",
    "    print(\"Covered EMPTY:\", len(covered_empty))\n",
    "    print(\"Missing:\", len(missing))\n",
    "\n",
    "    # Write audit CSVs (pandas optional)\n",
    "    audit_dir = out_root / \"bronze\" / \"audits\" / (\n",
    "        f\"ingest_date={ingest_date}\" if ingest_date else \"ingest_date=unknown\"\n",
    "    )\n",
    "    ensure_dir(audit_dir)\n",
    "    try:\n",
    "        import pandas as pd\n",
    "        pd.DataFrame(covered_ok, columns=[\"profile_key\",\"profile_id\",\"area_type_id\"]).to_csv(\n",
    "            audit_dir / \"covered_ok.csv\", index=False\n",
    "        )\n",
    "        pd.DataFrame(covered_empty, columns=[\"profile_key\",\"profile_id\",\"area_type_id\"]).to_csv(\n",
    "            audit_dir / \"covered_empty.csv\", index=False\n",
    "        )\n",
    "        pd.DataFrame(missing, columns=[\"profile_key\",\"profile_id\",\"area_type_id\"]).to_csv(\n",
    "            audit_dir / \"missing.csv\", index=False\n",
    "        )\n",
    "    except Exception:\n",
    "        def w(path, rows, hdr):\n",
    "            with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(\",\".join(hdr) + \"\\n\")\n",
    "                for r in rows:\n",
    "                    f.write(\",\".join(\"\" if (x is None) else str(x) for x in r) + \"\\n\")\n",
    "        w(audit_dir / \"covered_ok.csv\", covered_ok, [\"profile_key\",\"profile_id\",\"area_type_id\"])\n",
    "        w(audit_dir / \"covered_empty.csv\", covered_empty, [\"profile_key\",\"profile_id\",\"area_type_id\"])\n",
    "        w(audit_dir / \"missing.csv\", missing, [\"profile_key\",\"profile_id\",\"area_type_id\"])\n",
    "\n",
    "    print(\"Audit written to:\", audit_dir)\n",
    "    return {\n",
    "        \"ingest_date\": ingest_date,\n",
    "        \"expected_pairs\": len(expected_keys),\n",
    "        \"covered_ok\": len(covered_ok),\n",
    "        \"covered_empty\": len(covered_empty),\n",
    "        \"missing\": len(missing),\n",
    "        \"audit_dir\": str(audit_dir),\n",
    "    }\n",
    "\n",
    "# ---------- Backfill meta for CSV-only partitions ----------\n",
    "def backfill_meta_for_csv_only(out_root: Path, ingest_date: str) -> int:\n",
    "    \"\"\"\n",
    "    Create _meta.json for any partition that has all_data_by_profile.csv.gz but no _meta.json.\n",
    "    Computes sha256 of UNCOMPRESSED CSV (streamed) and counts rows.\n",
    "    Returns number of meta files written.\n",
    "    \"\"\"\n",
    "    print(\"\\nBackfilling meta for CSV-only partitions (if any)…\")\n",
    "    data_root = out_root / \"bronze\" / \"data\"\n",
    "\n",
    "    def token(parts, key):\n",
    "        for p in parts:\n",
    "            if isinstance(p, str) and p.startswith(f\"{key}=\"):\n",
    "                return p.split(\"=\",1)[1]\n",
    "        return None\n",
    "\n",
    "    def sha256_and_rows_of_gzip_uncompressed(p: Path) -> Tuple[str, int]:\n",
    "        h = hashlib.sha256()\n",
    "        rows = 0\n",
    "        with gzip.open(p, \"rb\") as g:\n",
    "            while True:\n",
    "                chunk = g.read(1024 * 256)\n",
    "                if not chunk:\n",
    "                    break\n",
    "                h.update(chunk)\n",
    "                rows += chunk.count(b\"\\n\")\n",
    "        return h.hexdigest(), rows\n",
    "\n",
    "    written = 0\n",
    "    now_s = now_iso()\n",
    "\n",
    "    for csv_path in data_root.rglob(\"all_data_by_profile.csv.gz\"):\n",
    "        parts = list(csv_path.parts)\n",
    "        pkey = token(parts, \"profile_key\")\n",
    "        atid = token(parts, \"area_type_id\")\n",
    "        idate = token(parts, \"ingest_date\")\n",
    "        if not (pkey and atid and idate):\n",
    "            continue\n",
    "        if ingest_date and idate != ingest_date:\n",
    "            continue\n",
    "        meta_path = csv_path.parent / \"_meta.json\"\n",
    "        if meta_path.exists():\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            sha, rows = sha256_and_rows_of_gzip_uncompressed(csv_path)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] could not read {csv_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "        if rows <= 1:\n",
    "            # header-only → mark empty and remove CSV for consistency\n",
    "            try:\n",
    "                csv_path.unlink()\n",
    "            except Exception:\n",
    "                pass\n",
    "            write_json(meta_path, {\n",
    "                \"endpoint\": None,\n",
    "                \"params\": None,\n",
    "                \"profile\": {\"Key\": pkey},\n",
    "                \"area_type_id\": int(atid),\n",
    "                \"parent_area_type_id\": None,\n",
    "                \"empty\": True,\n",
    "                \"reason\": \"header-only discovered during backfill\",\n",
    "                \"fetched_at\": now_s\n",
    "            })\n",
    "        else:\n",
    "            write_json(meta_path, {\n",
    "                \"endpoint\": None,\n",
    "                \"params\": None,\n",
    "                \"profile\": {\"Key\": pkey},\n",
    "                \"area_type_id\": int(atid),\n",
    "                \"parent_area_type_id\": None,\n",
    "                \"file\": \"all_data_by_profile.csv.gz\",\n",
    "                \"sha256_uncompressed\": sha,\n",
    "                \"row_count_estimate\": rows,\n",
    "                \"fetched_at\": now_s,\n",
    "                \"note\": \"backfilled_meta\"\n",
    "            })\n",
    "        written += 1\n",
    "\n",
    "    print(f\"Backfilled meta files: {written}\")\n",
    "    return written\n",
    "\n",
    "# ---------- S3 upload (bronze/ingest_date=...) ----------\n",
    "def s3_upload_bronze_ingest(out_root: Path, ingest_date: str,\n",
    "                            bucket: str, prefix: str,\n",
    "                            clean_first: bool = False,\n",
    "                            max_workers: int = 8):\n",
    "    import boto3\n",
    "    from boto3.s3.transfer import TransferConfig\n",
    "\n",
    "    s3 = boto3.client(\"s3\")\n",
    "    prefix = (prefix or \"\").strip(\"/\")\n",
    "    base_local = out_root / \"bronze\"\n",
    "    base_s3 = f\"{prefix}/bronze\" if prefix else \"bronze\"\n",
    "    ingest_marker = f\"ingest_date={ingest_date}\"\n",
    "\n",
    "    # optional clean (delete keys for this ingest_date)\n",
    "    if clean_first:\n",
    "        print(f\"\\n[ S3 CLEAN ] Deleting existing s3://{bucket}/{base_s3}/**/{ingest_marker}/**\")\n",
    "        token = None\n",
    "        to_delete = []\n",
    "        while True:\n",
    "            kw = dict(Bucket=bucket, Prefix=f\"{base_s3}/\")\n",
    "            if token:\n",
    "                kw[\"ContinuationToken\"] = token\n",
    "            resp = s3.list_objects_v2(**kw)\n",
    "            for it in resp.get(\"Contents\", []) or []:\n",
    "                key = it[\"Key\"]\n",
    "                if f\"/{ingest_marker}/\" in key:\n",
    "                    to_delete.append({\"Key\": key})\n",
    "                    if len(to_delete) == 1000:\n",
    "                        s3.delete_objects(Bucket=bucket, Delete={\"Objects\": to_delete})\n",
    "                        to_delete = []\n",
    "            token = resp.get(\"NextContinuationToken\")\n",
    "            if not token:\n",
    "                break\n",
    "        if to_delete:\n",
    "            s3.delete_objects(Bucket=bucket, Delete={\"Objects\": to_delete})\n",
    "        print(\"[ S3 CLEAN ] Done.\")\n",
    "\n",
    "    print(f\"\\n[ S3 UPLOAD ] Uploading bronze ingest_date={ingest_date} → s3://{bucket}/{base_s3}/\")\n",
    "    files = []\n",
    "    for p in base_local.rglob(\"*\"):\n",
    "        if p.is_file() and f\"/{ingest_marker}/\" in str(p.as_posix()):\n",
    "            rel = p.relative_to(out_root)\n",
    "            s3_key = f\"{prefix}/{rel.as_posix()}\" if prefix else rel.as_posix()\n",
    "            files.append((p, s3_key))\n",
    "\n",
    "    print(f\"Files to upload: {len(files)}\")\n",
    "\n",
    "    def mime_for(path: Path) -> Dict[str, str]:\n",
    "        name = path.name.lower()\n",
    "        if name.endswith(\".csv.gz\"):\n",
    "            return {\"ContentType\": \"text/csv\", \"ContentEncoding\": \"gzip\"}\n",
    "        if name.endswith(\".json.gz\"):\n",
    "            return {\"ContentType\": \"application/json\", \"ContentEncoding\": \"gzip\"}\n",
    "        if name.endswith(\".json\"):\n",
    "            return {\"ContentType\": \"application/json\"}\n",
    "        return {\"ContentType\": \"application/octet-stream\"}\n",
    "\n",
    "    cfg = TransferConfig(\n",
    "        multipart_threshold=8 * 1024 * 1024,\n",
    "        max_concurrency=max_workers,\n",
    "        multipart_chunksize=8 * 1024 * 1024,\n",
    "        use_threads=True,\n",
    "    )\n",
    "\n",
    "    def _upload_one(p: Path, key: str):\n",
    "        extra = mime_for(p)\n",
    "        s3.upload_file(str(p), bucket, key, ExtraArgs=extra, Config=cfg)\n",
    "        return key\n",
    "\n",
    "    uploaded = 0\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "        futs = [ex.submit(_upload_one, p, k) for (p, k) in files]\n",
    "        for f in tqdm(as_completed(futs), total=len(futs), desc=\"S3 Upload\"):\n",
    "            try:\n",
    "                f.result()\n",
    "                uploaded += 1\n",
    "            except Exception as e:\n",
    "                print(\"[ERROR] upload failed:\", e)\n",
    "\n",
    "    print(f\"[ S3 UPLOAD ] Uploaded {uploaded}/{len(files)} files.\")\n",
    "\n",
    "# ---------- MAIN ----------\n",
    "def main():\n",
    "    out_root = OUTPUT_DIR\n",
    "    ensure_dir(out_root)\n",
    "    t0 = time.time()\n",
    "\n",
    "    # (optional) metadata snapshot\n",
    "    if DUMP_METADATA:\n",
    "        print(\"Dumping Bronze metadata…\")\n",
    "        dump_bronze_metadata(out_root, INGEST_DATE, include_area_lists=INCLUDE_AREAS_LISTS)\n",
    "\n",
    "    # profiles (with cache fallback)\n",
    "    try:\n",
    "        profiles = fetch_profiles()\n",
    "    except Exception as e:\n",
    "        print(\"[WARN] /api/profiles failed; using cached profiles.json if available →\", e)\n",
    "        profiles = load_cached_profiles(out_root)\n",
    "        if not profiles:\n",
    "            raise RuntimeError(\"Could not fetch /api/profiles and no cached profiles.json found.\")\n",
    "\n",
    "    # selection\n",
    "    if ONLY_PROFILES:\n",
    "        wanted = {p.lower() for p in ONLY_PROFILES}\n",
    "        profiles = [p for p in profiles if (p.get(\"Key\") or slugify(p[\"Name\"])).lower() in wanted]\n",
    "\n",
    "    # build tasks using per-profile area types (with cache fallback)\n",
    "    tasks = []\n",
    "    at_cache_misses = 0\n",
    "    for p in profiles:\n",
    "        try:\n",
    "            ats = fetch_area_types_for_profile(p[\"Id\"])\n",
    "        except Exception:\n",
    "            at_cache_misses += 1\n",
    "            ats = load_cached_area_types_for_profile(out_root, p[\"Id\"])\n",
    "        ids = []\n",
    "        for a in ats or []:\n",
    "            atid = a.get(\"Id\", a.get(\"AreaTypeId\"))\n",
    "            if isinstance(atid, int):\n",
    "                ids.append(atid)\n",
    "        for atid in sorted(set(ids)):\n",
    "            tasks.append((p, atid))\n",
    "\n",
    "    print(f\"Planned tasks: {len(tasks)} from {len(profiles)} profiles (AT cache misses: {at_cache_misses})\")\n",
    "\n",
    "    # run main pass\n",
    "    results = []\n",
    "    started = time.time()\n",
    "\n",
    "    def _run_task(p, atid):\n",
    "        return download_profile_area_csv(p, atid, out_root, INGEST_DATE, PARENT_AREA_TYPE_ID)\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:\n",
    "        futs = []\n",
    "        for (p, atid) in tasks:\n",
    "            futs.append(ex.submit(_run_task, p, atid))\n",
    "            time.sleep(random.uniform(*GENTLE_JITTER))\n",
    "        for f in tqdm(as_completed(futs), total=len(futs), desc=\"Bronze CSV\"):\n",
    "            try:\n",
    "                results.append(f.result())\n",
    "            except Exception as e:\n",
    "                reason, code, detail = classify_exception(e)\n",
    "                results.append({\"error\": detail, \"reason\": reason, \"status\": code})\n",
    "\n",
    "    dur_main = time.time() - started\n",
    "    ok_main = sum(1 for r in results if r and r.get(\"ok\"))\n",
    "    empty_main = sum(1 for r in results if r and r.get(\"empty\"))\n",
    "    err_main = [r for r in results if r and r.get(\"error\")]\n",
    "    print(f\"Main pass → OK:{ok_main}  Empties:{empty_main}  Errors:{len(err_main)}  (in {fmt_secs(dur_main)})\")\n",
    "\n",
    "    if err_main:\n",
    "        by_reason = {}\n",
    "        for r in err_main:\n",
    "            by_reason[r.get(\"reason\", \"other\")] = by_reason.get(r.get(\"reason\", \"other\"), 0) + 1\n",
    "        print(\"Error breakdown:\", \", \".join(f\"{k}:{v}\" for k, v in sorted(by_reason.items())))\n",
    "\n",
    "    # build failures list (with ids)\n",
    "    failed_pairs = sorted({\n",
    "        (r.get(\"profile_id\"), r.get(\"area_type_id\"))\n",
    "        for r in err_main if r.get(\"profile_id\") and r.get(\"area_type_id\")\n",
    "    })\n",
    "    print(\"Failures to retry:\", len(failed_pairs))\n",
    "    if failed_pairs:\n",
    "        print(\"Retrying pairs:\")\n",
    "        for (pid, atid) in failed_pairs:\n",
    "            p = next((pp for pp in profiles if pp[\"Id\"] == pid), None)\n",
    "            key = (p.get(\"Key\") or slugify(p[\"Name\"])) if p else \"?\"\n",
    "            print(f\"  - profile_id={pid} key={key} area_type_id={atid}\")\n",
    "\n",
    "    profiles_by_id = {p[\"Id\"]: p for p in profiles}\n",
    "\n",
    "    # retry fallbacks\n",
    "    retry_outcomes = []\n",
    "    started_retry = time.time()\n",
    "    with ThreadPoolExecutor(max_workers=RETRY_MAX_WORKERS) as ex:\n",
    "        futs = []\n",
    "        for (pid, atid) in failed_pairs:\n",
    "            p = profiles_by_id.get(pid)\n",
    "            if not p:\n",
    "                continue\n",
    "            futs.append(ex.submit(\n",
    "                recover_profile_area_into_same_file, p, atid, out_root, INGEST_DATE, PARENT_AREA_TYPE_ID\n",
    "            ))\n",
    "            time.sleep(random.uniform(*GENTLE_JITTER))\n",
    "\n",
    "        for f in tqdm(as_completed(futs), total=len(futs), desc=\"Retry fallbacks\"):\n",
    "            try:\n",
    "                retry_outcomes.append(f.result())\n",
    "            except Exception as e:\n",
    "                reason, code, detail = classify_exception(e)\n",
    "                retry_outcomes.append({\"error\": detail, \"reason\": reason, \"status\": code})\n",
    "\n",
    "    dur_retry = time.time() - started_retry\n",
    "    ok_retry = sum(1 for r in retry_outcomes if r and r.get(\"ok\"))\n",
    "    empty_retry = sum(1 for r in retry_outcomes if r and r.get(\"empty\"))\n",
    "    err_retry = [r for r in retry_outcomes if r and r.get(\"error\")]\n",
    "    print(f\"Retry pass → OK:{ok_retry}  Empties:{empty_retry}  Errors:{len(err_retry)}  (in {fmt_secs(dur_retry)})\")\n",
    "    if err_retry:\n",
    "        by_reason_r = {}\n",
    "        for r in err_retry:\n",
    "            by_reason_r[r.get(\"reason\", \"other\")] = by_reason_r.get(r.get(\"reason\", \"other\"), 0) + 1\n",
    "        print(\"Retry error breakdown:\", \", \".join(f\"{k}:{v}\" for k, v in sorted(by_reason_r.items())))\n",
    "\n",
    "    # audit + backfill meta\n",
    "    audit = bronze_completeness_audit(out_root, INGEST_DATE)\n",
    "    backfilled = backfill_meta_for_csv_only(out_root, INGEST_DATE)\n",
    "\n",
    "    # write manifest\n",
    "    manifest = {\n",
    "        \"ingest_date\": INGEST_DATE,\n",
    "        \"created_at\": now_iso(),\n",
    "        \"ok_main\": ok_main,\n",
    "        \"empty_main\": empty_main,\n",
    "        \"errors_main\": len(err_main),\n",
    "        \"errors_main_breakdown\": {\n",
    "            r: sum(1 for x in err_main if x.get(\"reason\") == r)\n",
    "            for r in set(x.get(\"reason\", \"other\") for x in err_main)\n",
    "        },\n",
    "        \"ok_retry\": ok_retry,\n",
    "        \"empty_retry\": empty_retry,\n",
    "        \"errors_retry\": len(err_retry),\n",
    "        \"errors_retry_breakdown\": {\n",
    "            r: sum(1 for x in err_retry if x.get(\"reason\") == r)\n",
    "            for r in set(x.get(\"reason\", \"other\") for x in err_retry)\n",
    "        },\n",
    "        \"audit\": audit,\n",
    "        \"backfilled_meta\": backfilled,\n",
    "        \"total_duration_sec\": round(time.time() - t0, 1),\n",
    "    }\n",
    "    write_json(out_root / \"bronze\" / f\"manifest_ingest_date={INGEST_DATE}.json\", manifest)\n",
    "    print(\"Manifest written. Total elapsed:\", fmt_secs(time.time() - t0))\n",
    "\n",
    "    # upload to S3\n",
    "    if DO_UPLOAD_TO_S3 and S3_BUCKET:\n",
    "        s3_upload_bronze_ingest(\n",
    "            out_root,\n",
    "            INGEST_DATE,\n",
    "            bucket=S3_BUCKET,\n",
    "            prefix=S3_PREFIX,\n",
    "            clean_first=S3_CLEAN_INGEST_DATE_FIRST,\n",
    "            max_workers=S3_MAX_UPLOAD_WORKERS,\n",
    "        )\n",
    "        print(f\"✅ Uploaded bronze ingest {INGEST_DATE} to s3://{S3_BUCKET}/{S3_PREFIX.strip('/')}/bronze/\")\n",
    "    else:\n",
    "        print(\"S3 upload disabled; set DO_UPLOAD_TO_S3=True and configure S3_BUCKET/S3_PREFIX to enable.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0093db9e-a29e-4bff-8ad0-7992631dc854",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
